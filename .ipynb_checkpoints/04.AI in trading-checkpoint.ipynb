{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b13f4407-eb99-4403-900f-f97a256e2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af2c9c-8d63-46af-9849-afe90440fa3b",
   "metadata": {},
   "source": [
    "## AI in trading\n",
    "In this final section, I will explore the role of artificial intelligence (AI) in developing effective trading algorithms. So far, we have focused on traditional approaches such as momentum strategies and mean-reversion, where we applied concepts from physics, including the harmonic oscillator and stochastic differential equations. While these methods are grounded in theory, they rely heavily on assumptions—about market efficiency, distribution of returns, or stationarity—that may not always hold true in real-world financial data. These assumptions can introduce significant limitations and potential sources of error.\n",
    "\n",
    "In contrast, machine learning models can learn patterns directly from data without relying on rigid predefined assumptions. This data-driven approach allows for greater flexibility and adaptability, making AI a powerful tool for uncovering complex, non-linear relationships in financial markets that traditional models might miss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9973879-b375-4066-8b01-47b4e019930c",
   "metadata": {},
   "source": [
    "## Data prep and feature engineering\n",
    "Before training any machine learning model, careful data preparation and feature engineering are essential. In the context of trading, raw price data alone is often insufficient. Instead, we derive a variety of features that may capture underlying market dynamics. These include technical indicators such as moving averages, momentum, relative strength index (RSI), Bollinger bands, and volatility measures. The goal is to provide the model with inputs that reflect both short-term and long-term behavior of the asset. To avoid look-ahead bias, all features are computed using only past and present data available at the time of prediction. The data is then standardized to ensure that all features are on comparable scales, which helps neural networks converge more efficiently during training. Additionally, time-series data is split using walk-forward validation rather than random sampling, to better simulate real-world trading conditions and prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de83f41-f410-486c-80a6-67a01561beb3",
   "metadata": {},
   "source": [
    "## Bulding a classification model\n",
    "I will begin by building a classification modlel deciding if the price is going to go up or down on the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3feaa2e5-4400-4c45-8305-a2d1a328a200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mitko\\AppData\\Local\\Temp\\ipykernel_22684\\2053238188.py:2: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start='2010-01-01', end='2023-12-31')\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ticker = 'AAPL'\n",
    "df = yf.download(ticker, start='2010-01-01', end='2023-12-31')\n",
    "df = df[['Close']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe322c-3be5-442d-85bb-b63973b9c83c",
   "metadata": {},
   "source": [
    "After getting the data we need to engineer our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5e8024-5dce-42b9-a5aa-9fba3491a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature engineering\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
    "df['Volatility_10'] = df['Close'].rolling(window=10).std()\n",
    "\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df['RSI_14'] = compute_rsi(df['Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30248a35-25d1-4e31-9ec0-c65d25be4d9d",
   "metadata": {},
   "source": [
    "Now we need to define a target to train our neural network, drop all NaN values and define our features, scaling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575c1776-b0bd-4766-a9c1-fd54468b35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: 1 if next day's close > today's, else 0\n",
    "df['Target'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
    "\n",
    "# 4. Drop NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 5. Features and scaling\n",
    "features = ['Return', 'MA_5', 'Volatility_10', 'RSI_14']\n",
    "X = df[features].values\n",
    "y = df['Target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb7e8d3-d23d-46ad-b8a1-8ee77e2b1c98",
   "metadata": {},
   "source": [
    "Now we need to create sequences for our LSTM neural network since it works with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c29b593-a6bc-467d-ad5f-7bfd0ed995ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(X, y, seq_length=10):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        x_seq = X[i:i+seq_length]\n",
    "        y_seq = y[i+seq_length]\n",
    "        xs.append(x_seq)\n",
    "        ys.append(y_seq)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 10\n",
    "X_seq, y_seq = create_sequences(X_scaled, y, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e274e7e2-4956-4f7d-9e07-aaf70bc011c2",
   "metadata": {},
   "source": [
    "Now we need a train test split and to convert what we have to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f30194c-aa5d-4e73-9631-d92341154635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Train-test split (time based)\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# 8. Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eeddea1-68f4-45cc-9a27-b58b1c69fc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 0.6918, Train Acc: 0.5298 | Val Loss: 0.6920, Val Acc: 0.5257\n",
      "Epoch 5/30 | Train Loss: 0.6907, Train Acc: 0.5316 | Val Loss: 0.6948, Val Acc: 0.4929\n",
      "Epoch 10/30 | Train Loss: 0.6889, Train Acc: 0.5395 | Val Loss: 0.6995, Val Acc: 0.4914\n",
      "Epoch 15/30 | Train Loss: 0.6892, Train Acc: 0.5363 | Val Loss: 0.6935, Val Acc: 0.5386\n",
      "Epoch 20/30 | Train Loss: 0.6843, Train Acc: 0.5498 | Val Loss: 0.7109, Val Acc: 0.5414\n",
      "Epoch 25/30 | Train Loss: 0.6808, Train Acc: 0.5570 | Val Loss: 0.7089, Val Acc: 0.5357\n",
      "Epoch 30/30 | Train Loss: 0.6767, Train Acc: 0.5502 | Val Loss: 0.7164, Val Acc: 0.5357\n",
      "Test Accuracy: 0.5357\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False) \n",
    "\n",
    "# LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, features)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # take the output of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = LSTMClassifier(input_size=len(features))\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 11. Training loop\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        preds = (outputs > 0.5).float()\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs, y_test_tensor).item()\n",
    "        val_preds = (val_outputs > 0.5).float()\n",
    "        val_acc = (val_preds == y_test_tensor).float().mean().item()\n",
    "\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# 12. Final test accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_preds = (test_outputs > 0.5).float()\n",
    "    test_acc = (test_preds == y_test_tensor).float().mean().item()\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56392e12-1381-4ed7-aa72-733b7d2e3fd4",
   "metadata": {},
   "source": [
    "After creating the network and evaluating it we find out that the accuracy is no better than a random choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3719efcf-6d28-46d7-82be-62bb79b24c7e",
   "metadata": {},
   "source": [
    "Despite our efforts, the AI model failed to consistently predict stock movements with meaningful accuracy. This outcome highlights the inherent difficulty of applying machine learning to financial markets. Unlike many other domains, markets are highly noisy, non-stationary, and influenced by countless external, often unpredictable, factors. Even sophisticated models like LSTMs struggle because patterns in financial data are weak, unstable, and can change over time(regime shifts). Additionally, the historical data we use may contain biases or may simply not carry enough predictive signal. Without access to high-quality, diverse data and advanced techniques like ensemble learning, feature selection, or reinforcement learning, building a reliable trading AI is extremely challenging. This failure is not just due to the model or features but reflects a deeper truth: financial prediction is one of the hardest real-world tasks in AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7b945-9587-4dde-863d-f26de4f78a1d",
   "metadata": {},
   "source": [
    "## Adding more evaluation, L2 regularization, lowering learn rate, batch learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e92fd-1dc8-4a41-8cbf-951029d86475",
   "metadata": {},
   "source": [
    "Now I am going to improve the model. I am going to rewrite some code for the sake of making the changes visible. **In general we would improve on the first model directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1d24b4a-e8ee-422a-a77c-63ee80594a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mitko\\AppData\\Local\\Temp\\ipykernel_22060\\2910755713.py:3: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start='2010-01-01', end='2023-12-31')\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "ticker = 'AAPL'\n",
    "df = yf.download(ticker, start='2010-01-01', end='2023-12-31')\n",
    "# Use Close, and optionally Volume, High, Low for additional features\n",
    "df = df[['Close', 'Volume', 'High', 'Low']]  # Adjust if only Close is needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679740ef-3b2e-4f95-b894-58eedb058fb0",
   "metadata": {},
   "source": [
    "The next thing we will do is probably one of the most important. We are going to select our features. **We know that it does not matter how good our model is if we are using poor data and have engineered our features badly. Trash in trash out.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c211aabe-9243-456e-80c0-1c187083dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enhanced Features\n",
    "df['Return'] = df['Close'].pct_change()\n",
    "df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
    "df['Volatility_10'] = df['Close'].rolling(window=10).std()\n",
    "\n",
    "# RSI\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=period).mean()\n",
    "    loss = -delta.where(delta < 0, 0).rolling(window=period).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df['RSI_14'] = compute_rsi(df['Close'])\n",
    "\n",
    "# Momentum 5-day\n",
    "df['Momentum_5'] = df['Close'].diff(5)\n",
    "\n",
    "# MACD\n",
    "df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "\n",
    "# Bollinger Bands\n",
    "df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
    "df['BB_Std'] = df['Close'].rolling(window=20).std()\n",
    "df['BB_Upper'] = df['BB_Middle'] + 2 * df['BB_Std']\n",
    "df['BB_Lower'] = df['BB_Middle'] - 2 * df['BB_Std']\n",
    "df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']  # Width of the bands\n",
    "\n",
    "# Normalizing because we are using DL\n",
    "df['Volume_Norm'] = df['Volume'] / df['Volume'].rolling(window=20).mean()\n",
    "\n",
    "# Lagged features\n",
    "df['Lag_1'] = df['Close'].shift(1)\n",
    "df['Lag_3'] = df['Close'].shift(3)\n",
    "df['Lag_5'] = df['Close'].shift(5)\n",
    "\n",
    "# ATR\n",
    "def compute_atr(high, low, close, period=14):\n",
    "    tr1 = high - low\n",
    "    tr2 = abs(high - close.shift())\n",
    "    tr3 = abs(low - close.shift())\n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    return tr.rolling(window=period).mean()\n",
    "\n",
    "df['ATR_14'] = compute_atr(df['High'], df['Low'], df['Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc697706-9d0a-4462-abea-d30ea954195d",
   "metadata": {},
   "source": [
    "now we need a target, to drop all NaN and also scale, we are repeating steps from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "390930fd-6315-48e7-87c8-141f208bd152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Target: 1 if next day's close > today's, else 0\n",
    "df['Target'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
    "\n",
    "# 4. Drop NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 5. Features and scaling\n",
    "features = ['Return', 'MA_5', 'Volatility_10', 'RSI_14', 'Momentum_5', 'MACD', \n",
    "            'BB_Width', 'Volume_Norm', 'Lag_1', 'Lag_3', 'Lag_5', 'ATR_14']\n",
    "X = df[features].values\n",
    "y = df['Target'].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864aa84-fe57-43e7-a516-67fc30981f55",
   "metadata": {},
   "source": [
    "These sequences are needed because we are working with time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea46c09-5626-45ec-adc0-c08e1d261eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create sequences\n",
    "def create_sequences(X, y, seq_length=10):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        x_seq = X[i:i+seq_length]\n",
    "        y_seq = y[i+seq_length]\n",
    "        xs.append(x_seq)\n",
    "        ys.append(y_seq)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 10\n",
    "X_seq, y_seq = create_sequences(X_scaled, y, seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88868f4b-5b6b-4ea2-86c6-741d232d946a",
   "metadata": {},
   "source": [
    "What we we have to do now is to split the data and then convert it into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab81de4e-11ba-4abd-a36a-1532d7e96883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    " #Train-test split (time-based)\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# DataLoader. This is needed for batches\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4e666-9f28-4d32-ab2b-9fdd9e1fcfff",
   "metadata": {},
   "source": [
    "Now the interesting part, we will again define and train an LSTM-based binary classifier. The LSTMClassifier processes input sequences to capture temporal dependencies and predicts a probability using a sigmoid output. The ```train_and_evaluate``` function handles model training with class weighting (to address class imbalance), validation, early stopping, and evaluation using key metrics like accuracy, precision, recall, F1-score, and AUC. It also visualizes training/validation loss and plots the ROC curve to assess classification performance. The purpose is to try and make the model better while evaluating and seeing graphically what could be done better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4863fb97-a650-48b9-9776-26ce3ba026b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 1.3882, Train Acc: 0.5298 | Val Loss: 1.3892, Val Acc: 0.5257, Precision: 0.5257, Recall: 1.0000, F1: 0.6891, AUC: 0.4781\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 32),\n",
    "            nn.ReLU(), # After the first linear layer\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid() # After the last linear layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Training and evaluation function (with early stopping, class weighting)\n",
    "def train_and_evaluate(model, train_loader, X_test_tensor, y_test_tensor, criterion, optimizer, epochs, patience=5):\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch) # loss function telling us how wrong we are\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test_tensor)\n",
    "            val_loss = criterion(val_outputs, y_test_tensor).item()\n",
    "            val_preds = (val_outputs > 0.5).float()\n",
    "            val_probs = val_outputs\n",
    "            val_acc = (val_preds == y_test_tensor).float().mean().item()\n",
    "            val_precision = precision_score(y_test_tensor.numpy(), val_preds.numpy(), zero_division=0)\n",
    "            val_recall = recall_score(y_test_tensor.numpy(), val_preds.numpy(), zero_division=0)\n",
    "            val_f1 = f1_score(y_test_tensor.numpy(), val_preds.numpy(), zero_division=0)\n",
    "            val_auc = roc_auc_score(y_test_tensor.numpy(), val_probs.numpy())\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Precision: {val_precision:.4f}, \"\n",
    "                  f\"Recall: {val_recall:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "\n",
    "    # Final test evaluation and ROC curve\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        test_preds = (test_outputs > 0.5).float()\n",
    "        test_probs = test_outputs\n",
    "        test_acc = (test_preds == y_test_tensor).float().mean().item()\n",
    "        test_precision = precision_score(y_test_tensor.numpy(), test_preds.numpy(), zero_division=0)\n",
    "        test_recall = recall_score(y_test_tensor.numpy(), test_preds.numpy(), zero_division=0)\n",
    "        test_f1 = f1_score(y_test_tensor.numpy(), test_preds.numpy(), zero_division=0)\n",
    "        test_auc = roc_auc_score(y_test_tensor.numpy(), test_probs.numpy())\n",
    "\n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test_tensor.numpy(), test_probs.numpy())\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {test_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, \"\n",
    "          f\"F1: {test_f1:.4f}, AUC: {test_auc:.4f}\")\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return model, test_preds, train_losses, val_losses\n",
    "\n",
    "# Initialize and train model\n",
    "model = LSTMClassifier(input_size=len(features))\n",
    "# Class weights for imbalance\n",
    "class_weights = torch.tensor([1.0 / (y_train.mean() + 1e-6), 1.0 / (1 - y_train.mean() + 1e-6)]).mean()\n",
    "criterion = nn.BCELoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "epochs = 30\n",
    "patience = 5\n",
    "\n",
    "model, test_preds, train_losses, val_losses = train_and_evaluate(model, train_loader, X_test_tensor, y_test_tensor, criterion, optimizer, epochs, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e3845-fd91-4244-9816-76cef17f31f1",
   "metadata": {},
   "source": [
    "From the results it is vissible that we were not able to create a very reliable model. Despite our efforts the AUC is below 0.5 which is worse than a random choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071326b-d493-46e1-8169-609c5c65430c",
   "metadata": {},
   "source": [
    "## Applying unsupervised learning methods for financial data analysis\n",
    "In this part I am going to try and implement a well-know unsupervised machine learnign technique - **Principal component anaysis** for dimensionality reduction. I will try and analyze the daily returns of some tech stocks, identifying the **main sources of variation** in the data. , PCA helps identify the main sources of variation in the data. Each principal component represents a latent factor that explains a portion of the overall market behavior, allowing us to summarize complex relationships between assets into a few interpretable components. \n",
    "\n",
    "For the sake of this we will:\n",
    "- download data\n",
    "- extract ```adj close```\n",
    "- calculate the returns\n",
    "- standardize the returns\n",
    "- apply PCA\n",
    "- plot the cumulative explained variance\n",
    "- show how the principal components contribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a1823-99c6-4a81-b032-4d127c48082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tickers = ['MSFT', 'AAPL', 'GOOGL', 'AMZN', 'META']\n",
    "\n",
    "data = yf.download(tickers, start=\"2022-01-01\", end=\"2023-01-01\", auto_adjust=False)\n",
    "\n",
    "adj_close = data['Adj Close']\n",
    "\n",
    "returns = adj_close.pct_change().dropna()\n",
    "\n",
    "returns_scaled = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(returns_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.title(\"Cumulative Explained Variance by PCA Components\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Variance Explained\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "components_df = pd.DataFrame(pca.components_, columns=returns.columns)\n",
    "print(\"Principal Components (Eigenvectors):\")\n",
    "print(components_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05213e8a-899e-4c09-93c9-469d98c6b1d8",
   "metadata": {},
   "source": [
    "What do we see here?\n",
    "The cumulative variance graph shows us how much of the overall variation in the data is captured by each principal component. As we can see, the first component explains the majority of the variance - over 70% - meaning that most of the movement across these stocks can be described by this single factor. The remaining components each explain a smaller portion, with rapidly diminishing contribution.\n",
    "\n",
    "Next, let’s look at the principal components table, which shows how each stock contributes to each component. These are essentially the directions in the data that capture the most variation, and each row is a new axis - a linear combination of the original stocks.\n",
    "\n",
    "- In the first component, all five stocks - AAPL, AMZN, GOOGL, META, and MSFT - have similar positive weights. This suggests that this component captures the overall market or tech-sector trend, since all stocks tend to move in the same direction here.\n",
    "- In the second component, META has a very high loading (about 0.89), while the others are close to zero or negative. This implies that this component is largely driven by META-specific behavior, such as volatility or unique events that affected only Meta during this time period.\n",
    "- The third component is dominated by Amazon (AMZN), indicating another stock-specific factor.\n",
    "- In the later components, we start seeing more contrasting behavior. For example, the fourth component shows a strong positive contribution from Apple and a strong negative one from Google, meaning it captures scenarios where those two stocks diverge in price movement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3920920-f3d2-453f-9b80-6b58534c63f4",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- https://builtin.com/artificial-intelligence/ai-trading-stock-market-tech#:~:text=AI%20stock%20trading%20uses%20machine%20learning%2C%20sentiment%20analysis,efficiency%20to%20mitigate%20risks%20and%20provide%20higher%20returns.\n",
    "- https://www.sciencedirect.com/science/article/pii/S2772662221000102#:~:text=The%20stock%20market%20is%20turbulent%2C%20yet%20using%20artificial,as%20predictive%20analytics%20tools%20in%20the%20stock%20market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18524c-f406-4081-b461-b4a90455c3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
